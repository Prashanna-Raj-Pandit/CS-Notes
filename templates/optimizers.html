<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Optimizers in Deep Learning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', serif;
            line-height: 1.8;
            color: #2c3e50;
            background-color: #f8f9fa;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 60px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        h1 {
            font-size: 2.5em;
            color: #1a1a1a;
            margin-bottom: 30px;
            text-align: center;
            border-bottom: 3px solid #3498db;
            padding-bottom: 20px;
        }

        h2 {
            font-size: 2em;
            color: #2c3e50;
            margin-top: 50px;
            margin-bottom: 25px;
            border-left: 5px solid #3498db;
            padding-left: 15px;
        }

        h3 {
            font-size: 1.5em;
            color: #34495e;
            margin-top: 35px;
            margin-bottom: 20px;
        }

        p {
            margin-bottom: 20px;
            text-align: justify;
            font-size: 1.1em;
        }

        .equation {
            background-color: #ecf0f1;
            padding: 20px;
            margin: 30px 0;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            font-size: 1.2em;
            text-align: center;
            border-left: 4px solid #3498db;
        }

        .graph-container {
            margin: 40px 0;
            text-align: center;
        }

        .graph-container canvas {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 8px;
        }

        .note {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }

        .note strong {
            color: #856404;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
            font-size: 1em;
        }

        .comparison-table th,
        .comparison-table td {
            border: 1px solid #ddd;
            padding: 15px;
            text-align: left;
        }

        .comparison-table th {
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }

        .comparison-table tr:nth-child(even) {
            background-color: #f8f9fa;
        }

        .feature-box {
            background-color: #e8f4f8;
            border: 1px solid #3498db;
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
        }

        .problem-box {
            background-color: #ffe8e8;
            border: 1px solid #e74c3c;
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
        }

        ul, ol {
            margin-left: 40px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 12px;
            font-size: 1.05em;
        }

        code {
            background-color: #f4f4f4;
            padding: 3px 8px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #e74c3c;
        }

        .intro-section {
            font-size: 1.15em;
            line-height: 2;
            color: #555;
            margin-bottom: 40px;
            padding: 30px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Understanding Optimizers in Deep Learning: A Comprehensive Guide</h1>

        <div class="intro-section">
            <p>Imagine you're standing at the peak of a mountain range, blindfolded, and your goal is to reach the lowest valley. You can only feel the slope beneath your feet and take steps accordingly. This is essentially what an optimizer does in deep learning—it navigates the complex landscape of a neural network's loss function, searching for the lowest point where the model performs best. This journey from random initialization to optimal performance is at the heart of how machines learn, and optimizers are the engines that drive this learning process.</p>
        </div>

        <h2>What is an Optimizer?</h2>

        <p>An optimizer is an algorithm or method used to adjust the parameters (weights and biases) of a neural network to minimize the loss function. Think of the loss function as a measure of how wrong your model's predictions are. The lower the loss, the better your model performs. The optimizer's job is to find the set of parameters that produces the smallest possible loss value.</p>

        <p>In mathematical terms, if we have a loss function <code>L(θ)</code> where <code>θ</code> represents all the parameters of our neural network, the optimizer tries to find the value of <code>θ</code> that minimizes <code>L(θ)</code>. This is fundamentally an optimization problem, and the optimizer is our tool to solve it.</p>

        <p>During training, the optimizer receives gradient information from backpropagation—essentially, the direction and magnitude of change needed for each parameter to reduce the loss. The optimizer then decides how much to actually update each parameter. Different optimizers use different strategies to make these decisions, and that's what distinguishes one optimizer from another.</p>

        <h2>Why Do We Need Optimizers?</h2>

        <p>The need for optimizers becomes clear when we understand the scale and complexity of modern neural networks. A typical deep learning model might have millions or even billions of parameters. Manually adjusting these parameters is not just impractical—it's impossible. We need an automated, systematic way to find good parameter values.</p>

        <p>But why can't we just calculate the optimal parameters directly? The answer lies in the nature of neural networks. The relationship between parameters and loss is highly non-linear and non-convex. There's no closed-form solution, no simple equation we can solve to get the answer. The loss landscape resembles a complex mountain range with multiple peaks and valleys, saddle points, plateaus, and ravines. Finding the global minimum (the absolute lowest point) is computationally intractable for large networks.</p>

        <p>Optimizers provide several critical benefits. First, they make learning feasible by breaking down the enormous optimization problem into small, iterative steps. Instead of trying to find the perfect parameters in one shot, optimizers improve the parameters gradually, batch by batch, epoch by epoch. Second, they help navigate the treacherous loss landscape efficiently. A good optimizer can avoid getting stuck in poor local minima, traverse plateaus quickly, and maintain stability throughout training. Third, they enable us to work with massive datasets through techniques like stochastic gradient descent, where we update parameters using small batches of data rather than the entire dataset at once.</p>

        <p>Without effective optimizers, deep learning as we know it wouldn't exist. The models would either fail to learn, take impossibly long to train, or produce poor results. The renaissance of deep learning over the past decade has been powered not just by more data and compute, but also by better optimizers that can train increasingly complex models effectively.</p>

        <h2>The Mathematics Behind Optimization</h2>

        <p>Before diving into specific optimizer types, let's establish the mathematical foundation. The core concept is gradient descent, which forms the basis of most modern optimizers.</p>

        <h3>Gradient Descent Principle</h3>

        <p>The gradient of the loss function with respect to parameters tells us the direction of steepest ascent. To minimize the loss, we move in the opposite direction—the direction of steepest descent. Mathematically, this is expressed as:</p>

        <div class="equation">
            θ<sub>t+1</sub> = θ<sub>t</sub> - η∇L(θ<sub>t</sub>)
        </div>

        <p>Where <code>θ<sub>t</sub></code> represents the parameters at time step <code>t</code>, <code>η</code> (eta) is the learning rate that controls the step size, and <code>∇L(θ<sub>t</sub>)</code> is the gradient of the loss function at the current parameters. This simple equation is profoundly important—it's the fundamental update rule that enables neural networks to learn.</p>

        <p>The learning rate is a hyperparameter that deserves special attention. Too large, and the optimizer might overshoot the minimum, bouncing around chaotically or even diverging. Too small, and training becomes painfully slow, potentially getting stuck in suboptimal regions. Choosing the right learning rate is both an art and a science, and many modern optimizers include adaptive mechanisms to adjust learning rates automatically.</p>

        <h2>Types of Optimizers</h2>

        <p>Over the years, researchers have developed numerous optimizer variants, each designed to address specific challenges in training neural networks. Let's explore the most important ones, understanding their mechanics, strengths, and weaknesses.</p>

        <h3>1. Stochastic Gradient Descent (SGD)</h3>

        <p>Stochastic Gradient Descent is the grandfather of all optimizers. It's simple, robust, and surprisingly effective even today. The term "stochastic" refers to the use of random mini-batches of data rather than the entire dataset to compute gradients.</p>

        <div class="equation">
            θ<sub>t+1</sub> = θ<sub>t</sub> - η∇L(θ<sub>t</sub>; x<sup>(i:i+n)</sup>, y<sup>(i:i+n)</sup>)
        </div>

        <p>Here, instead of computing the gradient over all training examples, we compute it over a small batch <code>(x<sup>(i:i+n)</sup>, y<sup>(i:i+n)</sup>)</code>. This introduces noise into the gradient estimates, but it makes computation much faster and allows the optimizer to update parameters more frequently.</p>

        <div class="feature-box">
            <h4>Features of SGD:</h4>
            <ul>
                <li><strong>Simplicity:</strong> The algorithm is straightforward to implement and understand, requiring minimal computational overhead beyond the gradient computation itself.</li>
                <li><strong>Low Memory Footprint:</strong> SGD doesn't maintain any additional state or history, making it memory-efficient even for very large models.</li>
                <li><strong>Generalization:</strong> The noise introduced by mini-batch sampling can actually help, acting as a form of regularization that prevents overfitting and helps find flatter minima that generalize better.</li>
                <li><strong>Well-Understood Convergence:</strong> Extensive theoretical analysis exists for SGD, providing guarantees under certain conditions.</li>
            </ul>
        </div>

        <div class="problem-box">
            <h4>Problems with SGD:</h4>
            <ul>
                <li><strong>Choosing Learning Rate:</strong> Requires careful tuning and often benefits from learning rate schedules.</li>
                <li><strong>Same Learning Rate for All Parameters:</strong> All parameters are updated with the same learning rate, which may not be appropriate for parameters that receive vastly different gradient magnitudes.</li>
                <li><strong>Saddle Points:</strong> Can get stuck at saddle points where gradients are small but the point is not a minimum.</li>
                <li><strong>Slow Convergence:</strong> On complex loss surfaces, especially those with ravines or elongated valleys, SGD can oscillate and make slow progress.</li>
            </ul>
        </div>

        <h3>2. SGD with Momentum</h3>

        <p>Momentum is a simple but powerful modification to SGD. Inspired by physics, it adds a velocity term that accumulates gradients over time, allowing the optimizer to build up speed in consistent directions and dampen oscillations.</p>

        <div class="equation">
            v<sub>t</sub> = γv<sub>t-1</sub> + η∇L(θ<sub>t</sub>)<br><br>
            θ<sub>t+1</sub> = θ<sub>t</sub> - v<sub>t</sub>
        </div>

        <p>The momentum coefficient <code>γ</code> (typically 0.9) determines how much of the previous velocity is retained. This creates an exponentially weighted moving average of gradients. When gradients point in the same direction consistently, momentum accelerates. When they oscillate, momentum smooths them out.</p>

        <div class="feature-box">
            <h4>Features of Momentum:</h4>
            <ul>
                <li><strong>Faster Convergence:</strong> Accelerates in directions of consistent gradients, leading to faster progress toward minima.</li>
                <li><strong>Dampens Oscillations:</strong> Reduces zigzagging behavior in ravines or narrow valleys.</li>
                <li><strong>Escapes Local Minima:</strong> The accumulated velocity can help the optimizer roll past small local minima and saddle points.</li>
                <li><strong>Better for Deep Networks:</strong> Particularly effective in deep networks where gradients might have high variance.</li>
            </ul>
        </div>

        <div class="problem-box">
            <h4>Problems with Momentum:</h4>
            <ul>
                <li><strong>Overshooting:</strong> Can overshoot minima due to accumulated velocity, especially near convergence.</li>
                <li><strong>Additional Hyperparameter:</strong> Requires tuning the momentum coefficient γ in addition to the learning rate.</li>
                <li><strong>Still Uniform Learning Rate:</strong> Doesn't address the issue of different parameters needing different learning rates.</li>
            </ul>
        </div>

        <h3>3. Nesterov Accelerated Gradient (NAG)</h3>

        <p>Nesterov Momentum is a clever variant that looks ahead before computing gradients. Instead of computing the gradient at the current position, it computes it at the approximate future position determined by momentum.</p>

        <div class="equation">
            v<sub>t</sub> = γv<sub>t-1</sub> + η∇L(θ<sub>t</sub> - γv<sub>t-1</sub>)<br><br>
            θ<sub>t+1</sub> = θ<sub>t</sub> - v<sub>t</sub>
        </div>

        <p>The key difference is <code>∇L(θ<sub>t</sub> - γv<sub>t-1</sub>)</code>—we evaluate the gradient after applying momentum. This "look-ahead" makes NAG more responsive to the geometry of the loss surface.</p>

        <div class="feature-box">
            <h4>Features of NAG:</h4>
            <ul>
                <li><strong>Anticipatory Updates:</strong> Makes more informed updates by considering where momentum is taking us.</li>
                <li><strong>Better Theoretical Guarantees:</strong> Provably faster convergence rates for convex optimization.</li>
                <li><strong>Reduced Overshooting:</strong> Better at slowing down when approaching minima compared to standard momentum.</li>
            </ul>
        </div>

        <h3>4. Adagrad (Adaptive Gradient Algorithm)</h3>

        <p>Adagrad represents a major conceptual shift: different parameters can have different learning rates. It adapts the learning rate for each parameter based on the history of gradients for that parameter.</p>

        <div class="equation">
            G<sub>t</sub> = G<sub>t-1</sub> + (∇L(θ<sub>t</sub>))²<br><br>
            θ<sub>t+1</sub> = θ<sub>t</sub> - (η / √(G<sub>t</sub> + ε)) · ∇L(θ<sub>t</sub>)
        </div>

        <p>Here, <code>G<sub>t</sub></code> accumulates the squared gradients for each parameter. Parameters with large accumulated gradients get smaller learning rates, while parameters with small accumulated gradients get larger learning rates. The small constant <code>ε</code> (typically 1e-8) prevents division by zero.</p>

        <div class="feature-box">
            <h4>Features of Adagrad:</h4>
            <ul>
                <li><strong>Parameter-Specific Learning Rates:</strong> Automatically adapts learning rates individually for each parameter.</li>
                <li><strong>Excellent for Sparse Data:</strong> Particularly effective when dealing with sparse features, as infrequently occurring features get larger updates.</li>
                <li><strong>No Learning Rate Tuning:</strong> Works well with default learning rate (typically 0.01), reducing hyperparameter tuning burden.</li>
            </ul>
        </div>

        <div class="problem-box">
            <h4>Problems with Adagrad:</h4>
            <ul>
                <li><strong>Aggressive Learning Rate Decay:</strong> The accumulated squared gradients grow monotonically, causing learning rates to shrink continuously and eventually become infinitesimally small.</li>
                <li><strong>Premature Convergence:</strong> Training can stop making progress before reaching a good solution because learning rates have decayed too much.</li>
                <li><strong>Memory Requirements:</strong> Needs to maintain a running sum of squared gradients for every parameter.</li>
            </ul>
        </div>

        <h3>5. RMSprop (Root Mean Square Propagation)</h3>

        <p>RMSprop was developed to address Adagrad's aggressive learning rate decay. Instead of accumulating all past squared gradients, it maintains an exponentially decaying average, giving more weight to recent gradients.</p>

        <div class="equation">
            E[g²]<sub>t</sub> = βE[g²]<sub>t-1</sub> + (1-β)(∇L(θ<sub>t</sub>))²<br><br>
            θ<sub>t+1</sub> = θ<sub>t</sub> - (η / √(E[g²]<sub>t</sub> + ε)) · ∇L(θ<sub>t</sub>)
        </div>

        <p>The decay rate <code>β</code> (typically 0.9) controls how much history to retain. This modification allows RMSprop to continue learning even after many iterations, making it suitable for non-stationary problems and recurrent neural networks.</p>

        <div class="feature-box">
            <h4>Features of RMSprop:</h4>
            <ul>
                <li><strong>Resolves Adagrad's Decay Problem:</strong> Learning rates don't vanish, allowing continued learning throughout training.</li>
                <li><strong>Effective for RNNs:</strong> Works particularly well with recurrent neural networks and non-stationary objectives.</li>
                <li><strong>Automatic Learning Rate Scaling:</strong> Adapts to the scale of gradients automatically.</li>
                <li><strong>Works Well in Practice:</strong> Has become a default choice for many practitioners, especially in computer vision.</li>
            </ul>
        </div>

        <div class="problem-box">
            <h4>Problems with RMSprop:</h4>
            <ul>
                <li><strong>Additional Hyperparameter:</strong> Requires tuning the decay rate β.</li>
                <li><strong>Learning Rate Still Matters:</strong> While it adapts learning rates, the base learning rate still needs to be set appropriately.</li>
                <li><strong>No Bias Correction:</strong> Unlike Adam, RMSprop doesn't include bias correction for the running averages.</li>
            </ul>
        </div>

        <h3>6. Adam (Adaptive Moment Estimation)</h3>

        <p>Adam combines the best ideas from momentum and RMSprop. It maintains both a running average of gradients (first moment, like momentum) and a running average of squared gradients (second moment, like RMSprop). It has become the default optimizer for many deep learning applications.</p>

        <div class="equation">
            m<sub>t</sub> = β₁m<sub>t-1</sub> + (1-β₁)∇L(θ<sub>t</sub>)<br><br>
            v<sub>t</sub> = β₂v<sub>t-1</sub> + (1-β₂)(∇L(θ<sub>t</sub>))²<br><br>
            m̂<sub>t</sub> = m<sub>t</sub> / (1-β₁ᵗ)<br><br>
            v̂<sub>t</sub> = v<sub>t</sub> / (1-β₂ᵗ)<br><br>
            θ<sub>t+1</sub> = θ<sub>t</sub> - (η / √(v̂<sub>t</sub> + ε)) · m̂<sub>t</sub>
        </div>

        <p>The bias correction terms <code>m̂<sub>t</sub></code> and <code>v̂<sub>t</sub></code> are crucial. Since <code>m</code> and <code>v</code> are initialized at zero, they're biased toward zero early in training. The bias correction compensates for this, especially important in the first few iterations. Default values are <code>β₁=0.9</code>, <code>β₂=0.999</code>, <code>ε=1e-8</code>, and <code>η=0.001</code>.</p>

        <div class="feature-box">
            <h4>Features of Adam:</h4>
            <ul>
                <li><strong>Combines Multiple Techniques:</strong> Benefits from both momentum and adaptive learning rates.</li>
                <li><strong>Bias Correction:</strong> Corrects for initialization bias, making it reliable from the start of training.</li>
                <li><strong>Robust to Hyperparameters:</strong> Default hyperparameters work well across a wide range of problems.</li>
                <li><strong>Computationally Efficient:</strong> Despite maintaining two moment estimates, it's still efficient and practical.</li>
                <li><strong>Memory Efficient Per Parameter:</strong> Only needs to store two scalars per parameter.</li>
                <li><strong>Suitable for Large Datasets:</strong> Works well with massive datasets and high-dimensional parameter spaces.</li>
            </ul>
        </div>

        <div class="problem-box">
            <h4>Problems with Adam:</h4>
            <ul>
                <li><strong>Generalization Gap:</strong> Some studies suggest Adam can sometimes generalize worse than SGD with momentum, particularly in certain computer vision tasks.</li>
                <li><strong>Convergence Issues:</strong> In some scenarios, Adam can fail to converge to optimal solutions that SGD would find.</li>
                <li><strong>Memory Overhead:</strong> Requires storing both first and second moment estimates for all parameters.</li>
                <li><strong>Learning Rate Sensitivity:</strong> While robust, the learning rate still significantly affects performance.</li>
            </ul>
        </div>

        <h3>7. AdamW (Adam with Weight Decay)</h3>

        <p>AdamW is a modification of Adam that correctly implements L2 regularization (weight decay). The original Adam implementation couples weight decay with the adaptive learning rate, which can be suboptimal. AdamW decouples them.</p>

        <div class="equation">
            m<sub>t</sub> = β₁m<sub>t-1</sub> + (1-β₁)∇L(θ<sub>t</sub>)<br><br>
            v<sub>t</sub> = β₂v<sub>t-1</sub> + (1-β₂)(∇L(θ<sub>t</sub>))²<br><br>
            m̂<sub>t</sub> = m<sub>t</sub> / (1-β₁ᵗ)<br><br>
            v̂<sub>t</sub> = v<sub>t</sub> / (1-β₂ᵗ)<br><br>
            θ<sub>t+1</sub> = θ<sub>t</sub> - η[(m̂<sub>t</sub> / √(v̂<sub>t</sub> + ε)) + λθ<sub>t</sub>]
        </div>

        <p>The key difference is the weight decay term <code>λθ<sub>t</sub></code> is added directly to the update, rather than being part of the gradient. This seemingly small change has significant implications for how regularization interacts with the adaptive learning rate.</p>

        <div class="feature-box">
            <h4>Features of AdamW:</h4>
            <ul>
                <li><strong>Proper Regularization:</strong> Implements weight decay correctly, leading to better generalization.</li>
                <li><strong>Improved Performance:</strong> Often achieves better results than Adam, particularly in transformer models and NLP tasks.</li>
                <li><strong>Better Hyperparameter Decoupling:</strong> Learning rate and weight decay can be tuned more independently.</li>
            </ul>
        </div>

        <h3>8. Nadam (Nesterov-accelerated Adam)</h3>

        <p>Nadam combines Adam with Nesterov momentum, incorporating the look-ahead aspect of NAG into Adam's update rule.</p>

        <div class="equation">
            m<sub>t</sub> = β₁m<sub>t-1</sub> + (1-β₁)∇L(θ<sub>t</sub>)<br><br>
            v<sub>t</sub> = β₂v<sub>t-1</sub> + (1-β₂)(∇L(θ<sub>t</sub>))²<br><br>
            m̂<sub>t</sub> = m<sub>t</sub> / (1-β₁ᵗ)<br><br>
            v̂<sub>t</sub> = v<sub>t</sub> / (1-β₂ᵗ)<br><br>
            θ<sub>t+1</sub> = θ<sub>t</sub> - (η / √(v̂<sub>t</sub> + ε)) · (β₁m̂<sub>t</sub> + [(1-β₁)∇L(θ<sub>t</sub>)]/(1-β₁ᵗ))
        </div>

        <div class="feature-box">
            <h4>Features of Nadam:</h4>
            <ul>
                <li><strong>Faster Convergence:</strong> Often converges faster than Adam due to the look-ahead mechanism.</li>
                <li><strong>Best of Both Worlds:</strong> Combines the benefits of Nesterov momentum with adaptive learning rates.</li>
            </ul>
        </div>

        <h2>Comparative Analysis</h2>

        <table class="comparison-table">
            <thead>
                <tr>
                    <th>Optimizer</th>
                    <th>Key Strength</th>
                    <th>Best Use Case</th>
                    <th>Memory Cost</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>SGD</td>
                    <td>Simplicity, good generalization</td>
                    <td>Computer vision with proper tuning</td>
                    <td>Low</td>
                </tr>
                <tr>
                    <td>SGD + Momentum</td>
                    <td>Faster convergence than plain SGD</td>
                    <td>CNNs, image classification</td>
                    <td>Low</td>
                </tr>
                <tr>
                    <td>NAG</td>
                    <td>Anticipatory updates</td>
                    <td>Convex optimization problems</td>
                    <td>Low</td>
                </tr>
                <tr>
                    <td>Adagrad</td>
                    <td>Great for sparse data</td>
                    <td>NLP, sparse features</td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td>RMSprop</td>
                    <td>Non-stationary objectives</td>
                    <td>RNNs, online learning</td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td>Adam</td>
                    <td>Robust, general-purpose</td>
                    <td>Most deep learning tasks</td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td>AdamW</td>
                    <td>Better regularization</td>
                    <td>Transformers, NLP models</td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td>Nadam</td>
                    <td>Fast convergence</td>
                    <td>Tasks needing quick training</td>
                    <td>Medium</td>
                </tr>
            </tbody>
        </table>

        <h2>Understanding Convergence Behavior</h2>

        <p>Different optimizers navigate the loss landscape in distinct ways. Imagine training a model on a simple quadratic loss function—a bowl-shaped surface. SGD would take a somewhat winding path down the bowl, its trajectory influenced by the noise in mini-batch gradients. With momentum, the optimizer builds up speed and cuts across the bowl more directly, arriving at the bottom faster. Adam would adjust its step size as it goes, taking larger steps when far from the minimum and smaller, more careful steps as it approaches.</p>

        <p>On more complex, non-convex loss surfaces typical of deep neural networks, these differences become even more pronounced. SGD might explore more of the loss landscape, potentially finding flatter minima that generalize better to unseen data. Adam might converge faster but sometimes to sharper minima. This is why practitioners often experiment with multiple optimizers for a given problem.</p>

        <div class="graph-container">
            <canvas id="convergenceGraph" width="700" height="400"></canvas>
            <p><em>Figure 1: Convergence comparison of different optimizers on a simple optimization problem</em></p>
        </div>

        <h2>How Optimizers Calculate Updates</h2>

        <p>Let's walk through a concrete example of how Adam calculates a parameter update. Suppose we're training a neural network and we're at iteration t=100. For a particular weight parameter, the current value is θ=0.5, and the gradient we just computed is ∇L=0.3.</p>

        <p><strong>Step 1: Update First Moment (Gradient Average)</strong><br>
        Starting with m₉₉=0.2, we compute:<br>
        m₁₀₀ = 0.9 × 0.2 + 0.1 × 0.3 = 0.18 + 0.03 = 0.21</p>